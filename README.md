# Transcribe

A CLI-based speech-to-text transcription application optimized for Apple Silicon that processes multi-track recordings into accurate transcripts with speaker identification and timestamps.

## ğŸ¯ Overview

Transcribe uses a 5-stage pipeline to convert FLAC audio files into polished transcripts, designed specifically for multi-speaker recordings like podcasts or meetings. The system leverages Apple Silicon optimization and modern Python tooling for fast, high-quality transcription.

## ğŸ“Š Pipeline Architecture

```
Audio Files â†’ [Stage 0] â†’ [Stage 1] â†’ [Stage 2] â†’ [Stage 3] â†’ [Stage 4] â†’ Final Transcript
              Bootstrap   Preprocess  Silero     Whisper      GPT
```

### Stage 0: Bootstrap
- **Purpose**: Download ML models and validate environment
- **Technology**: Hugging Face model downloads
- **Output**: Ready environment with all dependencies

### Stage 1: Audio Preprocess
- **Input**: Raw FLAC audio files (`inputs/<speaker-name>.flac`)
- **Purpose**: Convert to 16kHz mono WAV with noise filtering
- **Technology**: FFmpeg with parallel processing
- **Output**: Cleaned audio files (`outputs/audio-files-wav/`)

### Stage 2: Silero VAD (Voice Activity Detection)
- **Input**: Cleaned audio files
- **Purpose**: Detect speech segments and filter silence/noise
- **Technology**: Silero VAD with parallel processing per file
- **Output**: JSON files with speech timestamps (`outputs/silero-timestamps/`)

### Stage 3: Whisper Transcribe
- **Input**: Speech segments and audio files
- **Purpose**: Convert speech to text with word-level timestamps
- **Technology**: MLX Whisper (Apple Silicon optimized)
- **Output**: Raw transcripts with timestamps (`outputs/whisper-transcripts/`)

### Stage 4: GPT Processing
- **Input**: Raw transcripts from all speakers
- **Purpose**: Merge transcripts, clean hallucinations, format for readability
- **Technology**: OpenAI API for intelligent post-processing
- **Output**: Final polished transcripts (`outputs/gpt-cleanup/`)

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd transcribe

# Install dependencies
uv sync

# Create environment file
cp .env.template .env
# Edit .env with your API keys:
# OPENAI_API_KEY=your_openai_api_key_here
# HUGGINGFACE_TOKEN=your_huggingface_token_here  # optional
```

### Basic Usage

```bash
# Validate installation and API keys
uv run transcribe validate

# Check pipeline status
uv run transcribe status

# Run full pipeline
uv run transcribe run

# Run specific stage
uv run transcribe run-stage preprocess

# Reset pipeline status
uv run transcribe reset

# Clean everything (reset + remove all inputs and outputs)
uv run transcribe clean

# Get help
uv run transcribe --help
```

### Input Files

Place your FLAC audio files in the `inputs/` directory:
```
inputs/
â”œâ”€â”€ speaker1.flac
â”œâ”€â”€ speaker2.flac
â””â”€â”€ speaker3.flac
```

The filename (without extension) is used as the speaker label in the final transcript.

## ğŸ”§ Configuration

### Optimal Settings (Pre-configured)

The system includes battle-tested configurations for best results:

**FFmpeg Audio Processing:**
```bash
ffmpeg -i input.flac -ar 16000 -af "highpass=f=60,agate=threshold=-45dB:ratio=10:attack=5:release=60" output.wav
```

**Silero VAD Parameters:**
- Threshold start: 0.6 (start speech detection)
- Threshold end: 0.4 (end speech detection)
- Min speech: 250ms (filter brief sounds)
- Min silence: 200ms (segment boundaries)
- Merge gap: 150ms (adjacent segment merging)

**Whisper Settings:**
- Model: small.en (optimal speed/accuracy balance)
- Temperature: 0.0 (deterministic output)
- Language: English with sentence-level segmentation

## ğŸ—ï¸ Architecture Benefits

- **Fast**: Process 3-hour recordings in â‰¤20 minutes (20x real-time speed)
- **Modular**: Each stage is independent and can be developed/tested separately
- **Resumable**: Can restart from any completed stage via `outputs/status.json`
- **Debuggable**: Inspect intermediate outputs at each stage
- **Apple Silicon Optimized**: Uses MLX Whisper for M1/M2 performance

## ğŸ“ Directory Structure

```
transcribe/
â”œâ”€â”€ inputs/                    # Input FLAC audio files
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ audio-files-wav/       # Stage 1: Converted WAV files
â”‚   â”œâ”€â”€ silero-timestamps/     # Stage 2: VAD speech segments
â”‚   â”œâ”€â”€ whisper-transcripts/   # Stage 3: Raw transcriptions
â”‚   â”œâ”€â”€ gpt-cleanup/           # Stage 4: Final transcripts
â”‚   â””â”€â”€ status.json            # Pipeline state tracking
â”œâ”€â”€ prompts/                   # GPT prompt templates
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ main.py               # CLI interface and orchestration
â”‚   â”œâ”€â”€ config.py             # Configuration management
â”‚   â”œâ”€â”€ ffmpeg_preprocess.py  # Audio preprocessing
â”‚   â”œâ”€â”€ vad_timestamp.py      # Silero VAD integration
â”‚   â”œâ”€â”€ whisper_transcribe.py # MLX Whisper processing
â”‚   â”œâ”€â”€ gpt_merge.py          # Multi-speaker merging
â”‚   â””â”€â”€ gpt_cleanup.py        # Final transcript cleanup
â””â”€â”€ tests/                    # Test directory (currently empty)
```

## ğŸ› ï¸ Development

### Code Quality

```bash
# Run linting
uv run ruff check

# Fix auto-fixable issues
uv run ruff check --fix

# Format code
uv run ruff format
```

### Technology Stack

- **Python 3.11+** with uv package manager
- **Audio Processing**: FFmpeg, librosa, soundfile
- **ML/AI**: MLX Whisper (Apple Silicon), Silero VAD, OpenAI API
- **CLI**: Click with Rich console interface
- **Configuration**: Pydantic with type validation
- **Async**: asyncio for I/O-bound operations

### Performance Targets

- **Speed**: 20x real-time processing (3-hour recording in â‰¤20 minutes)
- **Parallel Processing**: I/O-bound stages (preprocessing, VAD) run in parallel
- **Memory Efficient**: Sequential Whisper processing respects MacBook memory limits

## ğŸ“‹ Requirements

### System Dependencies
- FFmpeg (for audio processing)
- Python 3.11+
- Apple Silicon Mac (for MLX Whisper optimization)

### API Keys
- **OpenAI API Key**: Required for GPT-based transcript processing
- **Hugging Face Token**: Optional, for accessing gated models

### Python Dependencies
See `pyproject.toml` for complete list. Key dependencies:
- `mlx-whisper` - Apple Silicon optimized Whisper
- `silero-vad` - Voice activity detection
- `openai` - GPT API client
- `click` - CLI framework
- `rich` - Enhanced terminal output
- `pydantic` - Configuration validation

## ğŸ”„ Resume Functionality

The pipeline automatically tracks progress in `outputs/status.json`. If interrupted, simply run `uv run transcribe run` again to resume from the last completed stage.

Status tracking includes:
- Completion status for each stage
- File processing progress
- Error states and recovery information

## ğŸ“ Output Formats

Final transcripts are available in multiple formats:
- **JSON**: Machine-readable with full metadata
- **SRT**: Standard subtitle format with speaker labels

## ğŸ› Troubleshooting

### Common Issues

1. **Missing API Keys**: Ensure `.env` file contains valid OpenAI API key
2. **FFmpeg Not Found**: Install FFmpeg: `brew install ffmpeg`
3. **Memory Issues**: Large files may require breaking into smaller segments
4. **MLX Whisper Issues**: Falls back to standard Whisper automatically

### Getting Help

```bash
# Check system dependencies and API connectivity
uv run transcribe validate

# View detailed pipeline status
uv run transcribe status

# Reset if pipeline is in bad state
uv run transcribe reset

# Clean everything (reset + remove all files) for fresh start
uv run transcribe clean
```

## ğŸ“„ License

This project is designed for speech-to-text transcription tasks. Please ensure you have appropriate rights to process any audio content.
